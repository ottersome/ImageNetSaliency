# Intro 

Today I will do some research on the way to guide sparsification through 
the use of either variance of gradients or salient maps.

* 02:36 PM : Reading SeerNet: Predicting Convolutional Neural Network Feature-Map Sparsitythrough Low-Bit Quantization. They state that
"a *highly quantized* version of the original network
is sufficient in predicting the output". So they are basically 
saying that they can predict the appropriate sparsity level for a 
neural network ?

    * *Quantized*: to apporximiate or restrict in some certain way 

    * 02:41 PM : They seem to obtain their binary sparsity mask by using the 
    output feature amps.
    * 02:48 PM : They say sparsity may arise in different  places in a 
    neural network inference. 
    * **02:49 PM :** Speedup can also be obtained from focusing on 
    *input sparsity*. Same goes for *output sparsity*

    * 02:52 PM : *Output sparsity* can be predicted?
    * 02:56 PM : This paper seems to be mostly about predicting output
    sparsity(for feature maps?)
    * 02:57 PM : Though it seems like predicting output sparsity
    is good for accelerate CNN model inference.(but not actual training?)
    * **03:00 PM :** feature maps in CNN tend to have high sparsity.
    This is because the function that follow the last layer is ReLU
    and this will turn a lot of negataive numbers into zeros(this 
    sparsifying it)
    * Sparsity levels can reach the 40%-80% mark after ReLU.




